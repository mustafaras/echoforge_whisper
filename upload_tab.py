"""
Upload Tab - Dosya Y√ºkleme ve ƒ∞≈üleme Mod√ºl√º
Temiz ve d√ºzenli dosya y√ºkleme aray√ºz√º
"""

import streamlit as st
import os
import time
import gc
import tempfile
import json
from typing import Optional, Dict, Any
from collections import Counter
from datetime import datetime
from pathlib import Path

from config import (
    ALLOWED_FORMATS, FILE_SIZE_LIMITS, LANGUAGES, RESPONSE_FORMATS,
    get_text, get_current_language
)
from utils import (
    analyze_audio_file, create_waveform_plot, estimate_processing_time,
    analyze_text_with_ai, TranscriptionProcessor, MemoryManager
)
from database import save_transcription_to_db
from logger_config import transcription_logger


def _create_pdf_report(uploaded_file, transcript_text: str, ai_analysis: Optional[Dict], 
                      transcription_id: int, audio_info: Dict) -> Optional[str]:
    """AI analiz sonu√ßlarƒ±nƒ± otomatik PDF raporu olarak olu≈üturur ve kaydeder - Modern Tasarƒ±m"""
    
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import (SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, 
                                      PageBreak, KeepTogether, Image)
        from reportlab.lib import colors
        from reportlab.lib.units import cm, inch
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.graphics.shapes import Drawing, Rect
        from reportlab.graphics import renderPDF
        
        # PDF dosya yolu olu≈ütur - "data" klas√∂r√º
        pdf_dir = Path("data")
        pdf_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name_clean = "".join(c for c in uploaded_file.name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        pdf_path = pdf_dir / f"{file_name_clean}_{timestamp}_Premium_AI_Report.pdf"
        
        # PDF d√∂k√ºmanƒ± olu≈ütur - Modern margin'ler
        doc = SimpleDocTemplate(
            str(pdf_path), 
            pagesize=A4, 
            topMargin=1.5*cm, 
            bottomMargin=1.5*cm,
            leftMargin=2*cm,
            rightMargin=2*cm
        )
        
        # Unicode ve T√ºrk√ße karakter desteƒüi i√ßin geli≈ümi≈ü font y√∂netimi
        font_name = 'Helvetica'
        font_bold = 'Helvetica-Bold'
        
        try:
            # Unicode destekli fontlarƒ± dene
            import os
            
            # Windows sistem fontlarƒ±
            windows_fonts = [
                'C:/Windows/Fonts/arial.ttf',
                'C:/Windows/Fonts/calibri.ttf', 
                'C:/Windows/Fonts/tahoma.ttf'
            ]
            
            unicode_font_registered = False
            for font_path in windows_fonts:
                if os.path.exists(font_path):
                    try:
                        pdfmetrics.registerFont(TTFont('UnicodeFont', font_path))
                        font_name = 'UnicodeFont'
                        unicode_font_registered = True
                        break
                    except:
                        continue
            
            if not unicode_font_registered:
                # Fallback: Built-in fonts ile Unicode karakterleri
                font_name = 'Helvetica'
                
        except Exception as e:
            transcription_logger.warning(f"Font registration error: {e}")
            font_name = 'Helvetica'
        
        # Modern stil tanƒ±mlamalarƒ± - Premium tasarƒ±m
        styles = getSampleStyleSheet()
        
        # Ba≈ülƒ±k stilleri - Gradient efekti i√ßin renk paleti
        title_style = ParagraphStyle(
            'PremiumTitle',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=40,
            textColor=colors.HexColor('#1a365d'),  # Koyu mavi
            alignment=1,  # Center
            fontName=font_name,
            leading=30
        )
        
        subtitle_style = ParagraphStyle(
            'PremiumSubtitle',
            parent=styles['Heading2'],
            fontSize=18,
            spaceBefore=25,
            spaceAfter=15,
            textColor=colors.HexColor('#2b77ad'),  # Orta mavi
            fontName=font_name,
            leading=22
        )
        
        heading_style = ParagraphStyle(
            'PremiumHeading',
            parent=styles['Heading3'],
            fontSize=14,
            spaceBefore=20,
            spaceAfter=12,
            textColor=colors.HexColor('#2563eb'),  # Parlak mavi
            fontName=font_name,
            leading=18
        )
        
        normal_style = ParagraphStyle(
            'PremiumNormal',
            parent=styles['Normal'],
            fontSize=11,
            fontName=font_name,
            textColor=colors.HexColor('#1f2937'),  # Koyu gri
            leading=16,
            spaceAfter=6
        )
        
        # Highlighted text i√ßin √∂zel stil
        highlight_style = ParagraphStyle(
            'PremiumHighlight',
            parent=normal_style,
            backColor=colors.HexColor('#f0f9ff'),  # A√ßƒ±k mavi background
            borderColor=colors.HexColor('#3b82f6'),
            borderWidth=1,
            borderPadding=8
        )
        
        # T√ºrk√ße karakter i≈üleme fonksiyonu - Geli≈ümi≈ü
        def safe_text(text, preserve_structure=True):
            """T√ºrk√ße karakterleri g√ºvenli ≈üekilde i≈üler"""
            if not text:
                return "Metin bulunamadƒ±"
            
            # Unicode normalizasyon dene
            try:
                import unicodedata
                normalized = unicodedata.normalize('NFC', str(text))
                
                # Eƒüer Unicode font kayƒ±tlƒ±ysa, direkt kullan
                if 'UnicodeFont' in font_name:
                    return normalized
                
                # Deƒüilse, g√ºvenli karakterlere √ßevir
                turkish_map = {
                    '√ß': 'c', '√á': 'C', 'ƒü': 'g', 'ƒû': 'G',
                    'ƒ±': 'i', 'I': 'I', 'ƒ∞': 'I', 'i': 'i',
                    '√∂': 'o', '√ñ': 'O', '≈ü': 's', '≈û': 'S',
                    '√º': 'u', '√ú': 'U'
                }
                
                if preserve_structure:
                    # Yapƒ±yƒ± koruyarak √ßevir
                    result = normalized
                    for tr_char, en_char in turkish_map.items():
                        result = result.replace(tr_char, en_char)
                    return result
                else:
                    # ASCII'ye zorla √ßevir
                    return normalized.encode('ascii', 'ignore').decode('ascii')
                    
            except Exception as e:
                transcription_logger.warning(f"Text processing error: {e}")
                return str(text).encode('ascii', 'ignore').decode('ascii')
        
        content = []
        
        # PREMIUM BA≈ûLIK - Modern tasarƒ±m
        content.append(Spacer(1, 10))
        
        # Ana ba≈ülƒ±k
        title_text = safe_text("ü§ñ WhisperAI Premium Analiz Raporu")
        content.append(Paragraph(title_text, title_style))
        
        # Alt ba≈ülƒ±k - Dosya ismi
        subtitle_text = safe_text(f"üìÑ {uploaded_file.name}")
        content.append(Paragraph(subtitle_text, subtitle_style))
        content.append(Spacer(1, 25))
        
        # Premium header √ßizgisi
        content.append(Spacer(1, 10))
        
        # DOSYA Bƒ∞LGƒ∞LERƒ∞ - Modern card tasarƒ±mƒ±
        content.append(Paragraph(safe_text("üìã Dosya Bilgileri ve Teknik Detaylar"), heading_style))
        
        # Dosya bilgileri tablosu - Premium stil
        current_time = datetime.now()
        file_info_data = [
            ["üìÅ Dosya Adi", safe_text(uploaded_file.name)],
            ["üìÖ Tarih", current_time.strftime("%d/%m/%Y")],
            ["‚è∞ Saat", current_time.strftime("%H:%M:%S")],
            ["üÜî Rapor ID", f"#{transcription_id:06d}"],
            ["üíª Sistem", "WhisperAI Premium v2.0"]
        ]
        
        if audio_info:
            duration_min = audio_info.get('duration', 0) / 60
            size_mb = audio_info.get('file_size_bytes', 0) / (1024 * 1024)
            file_info_data.extend([
                ["üéµ Ses Suresi", f"{duration_min:.1f} dakika"],
                ["üìä Dosya Boyutu", f"{size_mb:.1f} MB"],
                ["üîä Sample Rate", f"{audio_info.get('sample_rate', 0):,} Hz"],
                ["üìª Kanal", "Stereo" if audio_info.get('channels', 1) > 1 else "Mono"],
                ["üìà Ortalama dB", f"{audio_info.get('avg_db', -50):.1f} dBFS"]
            ])
        
        # Premium tablo tasarƒ±mƒ±
        file_table = Table(file_info_data, colWidths=[5*cm, 11*cm])
        file_table.setStyle(TableStyle([
            # Header style
            ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#3b82f6')),  # Mavi header
            ('TEXTCOLOR', (0, 0), (0, -1), colors.white),
            ('FONTNAME', (0, 0), (0, -1), font_bold if 'UnicodeFont' not in font_name else font_name),
            ('FONTSIZE', (0, 0), (0, -1), 11),
            
            # Data style
            ('BACKGROUND', (1, 0), (1, -1), colors.HexColor('#f8fafc')),  # A√ßƒ±k gri
            ('TEXTCOLOR', (1, 0), (1, -1), colors.HexColor('#1f2937')),
            ('FONTNAME', (1, 0), (1, -1), font_name),
            ('FONTSIZE', (1, 0), (1, -1), 10),
            
            # Genel stil
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
            ('TOPPADDING', (0, 0), (-1, -1), 12),
            ('LEFTPADDING', (0, 0), (-1, -1), 15),
            ('RIGHTPADDING', (0, 0), (-1, -1), 15),
            
            # Sƒ±nƒ±rlar
            ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#e5e7eb')),
            ('ROUNDEDCORNERS', [5, 5, 5, 5]),
        ]))
        content.append(file_table)
        content.append(Spacer(1, 25))
        
        # TRANSKRƒ∞PSƒ∞YON METNƒ∞ - Premium tasarƒ±m
        content.append(Paragraph(safe_text("üìù Transkripsiyon Metni"), heading_style))
        
        # Metin uzunluƒüuna g√∂re sayfa b√∂lme
        safe_transcript = safe_text(transcript_text, preserve_structure=True)
        if not safe_transcript.strip():
            safe_transcript = safe_text("‚ö†Ô∏è Transkripsiyon metni i≈ülenirken hata olu≈ütu. T√ºrk√ße karakterler nedeniyle metin g√∂sterilemiyor.")
        
        # Paragraf stilleme - highlight box
        transcript_content = Paragraph(safe_transcript, highlight_style)
        content.append(KeepTogether([transcript_content]))
        content.append(Spacer(1, 25))
        
        # AI ANALƒ∞Z SONU√áLARI - Premium dashboard tasarƒ±mƒ±
        if ai_analysis:
            content.append(Paragraph(safe_text("ü§ñ Premium AI Analiz Dashboard"), heading_style))
            
            # ƒ∞statistikler kartƒ±
            text_stats = ai_analysis.get('text_statistics', {})
            if text_stats:
                content.append(Paragraph(safe_text("üìä Metin ƒ∞statistikleri"), subtitle_style))
                
                stats_data = [
                    ["üìù Kelime Sayƒ±sƒ±", f"{text_stats.get('word_count', 0):,}"],
                    ["üî§ Karakter Sayƒ±sƒ±", f"{text_stats.get('character_count', 0):,}"],
                    ["üìÑ C√ºmle Sayƒ±sƒ±", f"{text_stats.get('sentence_count', 0):,}"],
                    ["üó£Ô∏è Konu≈üma Hƒ±zƒ±", f"{text_stats.get('words_per_minute', 0):.0f} kelime/dakika"],
                    ["‚öñÔ∏è Ortalama Kelime/C√ºmle", f"{text_stats.get('average_words_per_sentence', 0):.1f}"],
                    ["üìö Okuma S√ºresi", f"{text_stats.get('reading_time_minutes', 0):.1f} dakika"]
                ]
                
                # Premium istatistik tablosu
                stats_table = Table(stats_data, colWidths=[7*cm, 9*cm])
                stats_table.setStyle(TableStyle([
                    # Gradient header
                    ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#1e40af')),
                    ('TEXTCOLOR', (0, 0), (0, -1), colors.white),
                    ('FONTNAME', (0, 0), (0, -1), font_bold if 'UnicodeFont' not in font_name else font_name),
                    ('FONTSIZE', (0, 0), (0, -1), 11),
                    
                    # Alternating row colors
                    ('ROWBACKGROUNDS', (1, 0), (1, -1), [colors.HexColor('#f1f5f9'), colors.HexColor('#e2e8f0')]),
                    ('TEXTCOLOR', (1, 0), (1, -1), colors.HexColor('#1f2937')),
                    ('FONTNAME', (1, 0), (1, -1), font_name),
                    ('FONTSIZE', (1, 0), (1, -1), 10),
                    
                    # Styling
                    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                    ('BOTTOMPADDING', (0, 0), (-1, -1), 14),
                    ('TOPPADDING', (0, 0), (-1, -1), 14),
                    ('LEFTPADDING', (0, 0), (-1, -1), 18),
                    ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#cbd5e1'))
                ]))
                content.append(stats_table)
                content.append(Spacer(1, 20))
            
            # Anahtar kelimeler - Premium chip tasarƒ±mƒ±
            keywords = ai_analysis.get('keywords', [])
            if keywords:
                content.append(Paragraph(safe_text("üè∑Ô∏è Anahtar Kelimeler"), subtitle_style))
                
                # Safe keywords i≈üleme
                safe_keywords = []
                for keyword in keywords[:15]:
                    safe_keyword = safe_text(str(keyword), preserve_structure=True)
                    if safe_keyword.strip():
                        safe_keywords.append(safe_keyword)
                
                if safe_keywords:
                    keywords_text = " ‚Ä¢ ".join(safe_keywords)
                    keyword_paragraph = Paragraph(keywords_text, normal_style)
                    content.append(keyword_paragraph)
                else:
                    content.append(Paragraph(safe_text("‚ö†Ô∏è Anahtar kelimeler i≈ülenirken hata olu≈ütu."), normal_style))
                
                content.append(Spacer(1, 20))
            
            # Duygu analizi - Modern card
            emotion_analysis = ai_analysis.get('emotion_analysis', '')
            if emotion_analysis and emotion_analysis != "Duygusal analiz yapƒ±lamadƒ±":
                content.append(Paragraph(safe_text("üí≠ Duygu Analizi"), subtitle_style))
                safe_emotion = safe_text(str(emotion_analysis), preserve_structure=True)
                emotion_paragraph = Paragraph(safe_emotion, highlight_style)
                content.append(emotion_paragraph)
                content.append(Spacer(1, 20))
            
            # √ñzet - Premium format
            summary = ai_analysis.get('summary', '')
            if summary and summary != '√ñzet bulunamadƒ±':
                content.append(Paragraph(safe_text("üìÑ ƒ∞√ßerik √ñzeti"), subtitle_style))
                safe_summary = safe_text(str(summary), preserve_structure=True)
                summary_paragraph = Paragraph(safe_summary, highlight_style)
                content.append(summary_paragraph)
                content.append(Spacer(1, 20))
            
            # Ana konular - Premium liste
            topics = ai_analysis.get('topics', [])
            if topics:
                content.append(Paragraph(safe_text("üéØ Ana Konular"), subtitle_style))
                safe_topics = []
                for topic in topics:
                    safe_topic = safe_text(str(topic), preserve_structure=True)
                    if safe_topic.strip():
                        safe_topics.append(safe_topic)
                
                if safe_topics:
                    topics_text = " | ".join(safe_topics)
                    topics_paragraph = Paragraph(topics_text, normal_style)
                    content.append(topics_paragraph)
                
                content.append(Spacer(1, 20))
            
            # En sƒ±k kullanƒ±lan kelimeler - Premium dashboard
            word_freq_data = ai_analysis.get('word_frequency', {})
            most_common = word_freq_data.get('most_common_words', [])
            if most_common:
                content.append(Paragraph(safe_text("üî• En Sƒ±k Kullanƒ±lan Kelimeler"), subtitle_style))
                
                freq_data = [["üèÜ Sƒ±ra", "üìù Kelime", "üî¢ Kullanƒ±m", "üìä Oran"]]
                word_count = text_stats.get('word_count', 1)
                
                for i, (word, count) in enumerate(most_common[:10], 1):
                    percentage = (count / word_count) * 100 if word_count > 0 else 0
                    safe_word = safe_text(str(word), preserve_structure=True)
                    if safe_word.strip():
                        freq_data.append([
                            f"#{i}", 
                            safe_word, 
                            f"{count}x", 
                            f"%{percentage:.1f}"
                        ])
                
                # Premium frekans tablosu
                if len(freq_data) > 1:  # Header + en az 1 data row
                    freq_table = Table(freq_data, colWidths=[2*cm, 5*cm, 3*cm, 3*cm])
                    freq_table.setStyle(TableStyle([
                        # Premium header
                        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#059669')),  # Ye≈üil header
                        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                        ('FONTNAME', (0, 0), (-1, 0), font_bold if 'UnicodeFont' not in font_name else font_name),
                        ('FONTSIZE', (0, 0), (-1, 0), 12),
                        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
                        
                        # Data rows - zebra striping
                        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#f8fafc')]),
                        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#374151')),
                        ('FONTNAME', (0, 1), (-1, -1), font_name),
                        ('FONTSIZE', (0, 1), (-1, -1), 10),
                        ('ALIGN', (0, 1), (0, -1), 'CENTER'),  # Sƒ±ra numarasƒ±
                        ('ALIGN', (1, 1), (1, -1), 'LEFT'),    # Kelime
                        ('ALIGN', (2, 1), (-1, -1), 'CENTER'), # Kullanƒ±m ve oran
                        
                        # Styling
                        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                        ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
                        ('TOPPADDING', (0, 0), (-1, -1), 12),
                        ('LEFTPADDING', (0, 0), (-1, -1), 10),
                        ('RIGHTPADDING', (0, 0), (-1, -1), 10),
                        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#d1d5db')),
                        
                        # Rounded corners effect
                        ('ROUNDEDCORNERS', [5, 5, 5, 5]),
                    ]))
                    content.append(freq_table)
                    content.append(Spacer(1, 25))
        
        # PREMIUM FOOTER - Modern branding
        content.append(Spacer(1, 40))
        content.append(PageBreak())  # Yeni sayfa footer i√ßin
        
        # Footer card
        footer_info = [
            ["üïí Rapor Tarihi", current_time.strftime("%d/%m/%Y %H:%M:%S")],
            ["‚öôÔ∏è AI Engine", "OpenAI Whisper + GPT-4 Turbo"],
            ["üè¢ Platform", "WhisperAI Multilingual Premium"],
            ["üìã Rapor Versiyonu", "v2.0 - Turkish Enhanced"],
            ["üîê G√ºvenlik", "End-to-End Encrypted Processing"]
        ]
        
        footer_table = Table(footer_info, colWidths=[4*cm, 12*cm])
        footer_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#6366f1')),  # Indigo
            ('TEXTCOLOR', (0, 0), (0, -1), colors.white),
            ('BACKGROUND', (1, 0), (1, -1), colors.HexColor('#f9fafb')),
            ('TEXTCOLOR', (1, 0), (1, -1), colors.HexColor('#111827')),
            ('FONTNAME', (0, 0), (-1, -1), font_name),
            ('FONTSIZE', (0, 0), (-1, -1), 9),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
            ('TOPPADDING', (0, 0), (-1, -1), 8),
            ('LEFTPADDING', (0, 0), (-1, -1), 12),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#e5e7eb'))
        ]))
        content.append(footer_table)
        
        # Signature
        signature_text = safe_text("¬© 2025 WhisperAI Premium - T√ºrk√ße Dil Desteƒüi Aktif")
        signature_para = Paragraph(signature_text, normal_style)
        content.append(Spacer(1, 15))
        content.append(signature_para)
        
        # PDF'i olu≈ütur - Premium kalite
        try:
            doc.build(content)
            transcription_logger.info(f"Premium PDF report created successfully: {pdf_path}")
            return str(pdf_path)
        except Exception as build_error:
            transcription_logger.error(f"PDF build error: {build_error}")
            # Fallback: Basit PDF olu≈ütur
            try:
                simple_content = [
                    Paragraph(safe_text("WhisperAI Transcription Report", preserve_structure=False), title_style),
                    Spacer(1, 20),
                    Paragraph(safe_text(f"File: {uploaded_file.name}", preserve_structure=False), normal_style),
                    Spacer(1, 10),
                    Paragraph(safe_text("Transcript:", preserve_structure=False), heading_style),
                    Paragraph(safe_text(transcript_text, preserve_structure=False), normal_style)
                ]
                doc.build(simple_content)
                return str(pdf_path)
            except Exception as fallback_error:
                transcription_logger.error(f"Fallback PDF creation also failed: {fallback_error}")
                return None
    
    except ImportError as import_err:
        transcription_logger.warning(f"ReportLab not installed: {import_err}")
        return None
    except Exception as e:
        transcription_logger.error(f"PDF creation error: {e}")
        return None


def _install_reportlab_if_needed():
    """ReportLab k√ºt√ºphanesini kontrol eder, yoksa kullanƒ±cƒ±yƒ± uyarƒ±r"""
    try:
        import reportlab
        return True
    except ImportError:
        st.warning("‚ö†Ô∏è PDF raporu olu≈üturmak i√ßin ReportLab k√ºt√ºphanesi gerekli. Y√ºklemek i√ßin: `pip install reportlab`")
        return False


def render_upload_tab(client, transcription_processor):
    """Dosya y√ºkleme tab'ƒ±nƒ± render eder"""
    
    # Temiz ba≈ülƒ±k
    st.markdown(f"""
    <div style="text-align: center; margin: 2rem 0;">
        <h1 style="color: #4a90e2; font-size: 2.2rem; margin-bottom: 1rem;">
            üéµ {get_text("upload_title")}
        </h1>
        <p style="color: #888; font-size: 1rem; margin-bottom: 0.5rem;">
            {get_text("upload_description")}
        </p>
        <p style="color: #666; font-size: 0.9rem;">
            <strong>{get_text("upload_formats")}</strong>
        </p>
    </div>
    """, unsafe_allow_html=True)

    # Dosya y√ºkleme alanƒ±
    uploaded_files = st.file_uploader(
        "",  # Bo≈ü label - √ß√ºnk√º √ºstte a√ßƒ±klama var
        type=ALLOWED_FORMATS, 
        accept_multiple_files=True,
        help=get_text("multiple_files_help"),
        label_visibility="collapsed"
    )

    if not uploaded_files:
        # Y√ºkleme alanƒ± bo≈üken bilgi g√∂ster
        st.markdown(f"""
        <div style="text-align: center; padding: 3rem; background: #1a1d23; border-radius: 12px; margin: 2rem 0;">
            <h3 style="color: #4a90e2; margin-bottom: 1rem;">üìÅ {get_text("drag_drop_files")}</h3>
            <p style="color: #888;">
                {get_text("supported_formats")}: MP3, WAV, M4A, MP4, MPEG4<br>
                {get_text("max_file_size")}: {FILE_SIZE_LIMITS["max_file_size"] // (1024*1024)} MB<br>
                {get_text("multiple_files_supported")}
            </p>
        </div>
        """, unsafe_allow_html=True)
        return

    # Dosyalar y√ºklendiyse i≈üleme ba≈üla
    st.markdown(f"""
    <div style="text-align: center; margin: 1.5rem 0;">
        <h3 style="color: #4a90e2;">üìä {len(uploaded_files)} {get_text("files_uploaded")}</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # Her dosyayƒ± i≈üle
    for i, uploaded_file in enumerate(uploaded_files):
        _process_single_file(uploaded_file, i, client, transcription_processor)


def _process_single_file(uploaded_file, file_index: int, client, transcription_processor):
    """Tek bir dosyayƒ± i≈üler"""
    
    st.markdown("---")
    
    # Dosya ba≈ülƒ±ƒüƒ±
    st.markdown(f"### üìÑ {uploaded_file.name}")
    
    # Dosya validasyonu
    if not _validate_file(uploaded_file):
        return
    
    # Ses analizi
    file_bytes = uploaded_file.getvalue()
    audio_info = _analyze_audio(uploaded_file.name, file_bytes)
    
    if not audio_info:
        return
    
    # Dosya bilgilerini g√∂ster
    _display_file_info(audio_info)
    
    # Transkripsiyon i≈ülemi
    _handle_transcription(uploaded_file, file_index, file_bytes, audio_info, client, transcription_processor)


def _validate_file(uploaded_file) -> bool:
    """Dosya validasyonu"""
    
    # Boyut kontrol√º
    if uploaded_file.size > FILE_SIZE_LIMITS["max_file_size"]:
        st.error(f"‚ùå {uploaded_file.name} {get_text('file_too_large')} "
                f"({FILE_SIZE_LIMITS['max_file_size'] // (1024*1024)} {get_text('mb_limit')})")
        return False
    
    # Format kontrol√º
    file_extension = os.path.splitext(uploaded_file.name)[1].lower()[1:]
    if file_extension not in ALLOWED_FORMATS:
        st.error(f"‚ùå {uploaded_file.name} {get_text('unsupported_format')}")
        return False
    
    return True


def _analyze_audio(file_name: str, file_bytes: bytes) -> Optional[Dict]:
    """Ses dosyasƒ± analizi"""
    
    with st.spinner(f"üîç {file_name} {get_text('analyzing')}..."):
        try:
            return analyze_audio_file(file_bytes, file_name)
        except Exception as e:
            st.error(f"‚ùå {get_text('audio_analysis_error')}: {str(e)}")
            return None


def _display_file_info(audio_info: Dict):
    """Dosya bilgilerini g√∂ster"""
    
    # 4 kolonlu bilgi g√∂sterimi
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        duration_min = audio_info['duration'] / 60
        st.metric("‚è±Ô∏è S√ºre", f"{duration_min:.1f} dk")
    
    with col2:
        size_mb = audio_info.get('file_size_bytes', 0) / (1024 * 1024)
        st.metric("üìä Boyut", f"{size_mb:.1f} MB")
    
    with col3:
        sample_rate = audio_info.get('sample_rate', 0)
        st.metric("üéµ Sample Rate", f"{sample_rate} Hz")
    
    with col4:
        channels = "Stereo" if audio_info.get('channels', 1) > 1 else "Mono"
        st.metric("üîä Kanal", channels)
    
    # Ses kalitesi deƒüerlendirmesi
    db_value = audio_info.get('avg_db', -50)
    if db_value > -12:
        quality = f"üü¢ {get_text('quality_high')} ({db_value:.1f} dBFS)"
    elif db_value > -20:
        quality = f"üü° {get_text('quality_medium')} ({db_value:.1f} dBFS)"
    else:
        quality = f"üî¥ {get_text('quality_low')} ({db_value:.1f} dBFS)"
    
    st.info(f"**{get_text('audio_quality')}:** {quality}")
    
    # Tahmini i≈ülem s√ºresi
    duration_minutes = audio_info['duration'] / 60  # saniyeyi dakikaya √ßevir
    estimated_time = estimate_processing_time(duration_minutes)
    st.info(f"**{get_text('estimated_processing_time')}:** {estimated_time}")


def _handle_transcription(uploaded_file, file_index: int, file_bytes: bytes, 
                         audio_info: Dict, client, transcription_processor):
    """Transkripsiyon i≈ülemini y√∂netir"""
    
    # Processing kontrol√º
    processing_key = f"processing_{file_index}"
    
    if st.session_state.get(processing_key, False):
        st.warning(f"‚è≥ {uploaded_file.name} i≈üleniyor...")
        return
    
    # ƒ∞≈ülem butonu
    if st.button(f"üöÄ {uploaded_file.name} {get_text('start_transcription')}", 
                key=f"process_btn_{file_index}",
                type="primary",
                use_container_width=True):
        
        st.session_state[processing_key] = True
        _perform_transcription(uploaded_file, file_index, file_bytes, audio_info, client, transcription_processor)


def _perform_transcription(uploaded_file, file_index: int, file_bytes: bytes,
                          audio_info: Dict, client, transcription_processor):
    """Ger√ßek transkripsiyon i≈ülemi"""
    
    try:
        # Sidebar'dan ayarlarƒ± al
        language_code = st.session_state.get('selected_language_code', None)
        response_format = st.session_state.get('response_format', 'text')
        enable_ai_analysis = st.session_state.get('enable_ai_analysis', True)
        ai_model = st.session_state.get('ai_model', 'gpt-4-turbo')
        
        # Progress placeholders
        progress_container = st.container()
        
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def progress_callback(message: str, percent: float):
                progress_bar.progress(percent / 100.0)
                status_text.info(f"üîÑ {message}")
            
            # Transkripsiyon
            status_text.info("üéµ Transkripsiyon ba≈ülƒ±yor...")
            result = transcription_processor.process_audio_file(
                file_bytes, 
                uploaded_file.name, 
                language_code,
                response_format,
                progress_callback
            )
            
            if not result or not result.get('transcript'):
                st.error(f"‚ùå {uploaded_file.name} {get_text('transcription_failed')}")
                return
            
            transcript_text = result['transcript']
            
            # AI Analiz - EN DETAYLI HALI
            ai_analysis = None
            if enable_ai_analysis and transcript_text.strip():
                status_text.info("ü§ñ Detaylƒ± AI analiz yapƒ±lƒ±yor (duygu, anahtar kelimeler, konu analizi)...")
                progress_bar.progress(0.85)
                
                try:
                    ai_analysis = analyze_text_with_ai(
                        transcript_text, 
                        client,
                        audio_info.get('duration', 0),
                        ai_model
                    )
                    
                    # AI analiz sonucunu zenginle≈ütir
                    if ai_analysis:
                        ai_analysis = _enhance_ai_analysis(ai_analysis, transcript_text, audio_info)
                        
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è AI analiz hatasƒ±: {str(e)}")
                    transcription_logger.error(f"AI analysis error: {e}")
            
            # Veritabanƒ±na kaydet
            status_text.info("üíæ Veritabanƒ±na kaydediliyor...")
            progress_bar.progress(0.95)
            
            transcription_id = save_transcription_to_db(
                uploaded_file.name,
                file_bytes,
                audio_info,
                language_code or 'auto',
                response_format,
                transcript_text,
                ai_analysis or {}
            )
            
            # Tamamlandƒ±
            progress_bar.progress(1.0)
            status_text.success(f"‚úÖ {uploaded_file.name} ba≈üarƒ±yla i≈ülendi!")
            
            # Session state'e minimal bilgi kaydet - sadece ID
            if transcription_id:
                # Sadece gerekli bilgiyi kaydet - session state ≈üi≈ümemesi i√ßin
                st.session_state[f"completed_{transcription_id}"] = {
                    'file_name': uploaded_file.name,
                    'completed_at': datetime.now().strftime("%H:%M:%S"),
                    'has_ai_analysis': bool(ai_analysis)
                }
            
            # Sonu√ßlarƒ± g√∂ster
            if transcription_id:
                _display_results(uploaded_file, transcript_text, ai_analysis, transcription_id, audio_info)
            else:
                st.error("‚ùå Veritabanƒ±na kayƒ±t ba≈üarƒ±sƒ±z oldu")
    
    except Exception as e:
        st.error(f"‚ùå {get_text('processing_error')}: {str(e)}")
        transcription_logger.error(f"Transcription error for {uploaded_file.name}: {e}")
    
    finally:
        # Processing flagini temizle
        if f"processing_{file_index}" in st.session_state:
            del st.session_state[f"processing_{file_index}"]
        
        # Bellek temizliƒüi
        MemoryManager.smart_cleanup_after_processing()


def _enhance_ai_analysis(ai_analysis: Dict, transcript_text: str, audio_info: Dict) -> Dict:
    """AI analiz sonucunu ek verilerle zenginle≈ütir"""
    
    try:
        # Temel metin istatistikleri ekle
        words = transcript_text.split()
        word_count = len(words)
        char_count = len(transcript_text)
        sentences = len([s for s in transcript_text.split('.') if s.strip()])
        
        # Ses verisi ile baƒülantƒ± kur
        duration_seconds = audio_info.get('duration', 0)
        duration_minutes = duration_seconds / 60 if duration_seconds > 0 else 0
        
        # Konu≈üma hƒ±zƒ± hesapla
        words_per_minute = word_count / duration_minutes if duration_minutes > 0 else 0
        
        # Ek metrikleri AI analizine ekle
        ai_analysis.update({
            'text_statistics': {
                'word_count': word_count,
                'character_count': char_count,
                'sentence_count': sentences,
                'average_words_per_sentence': word_count / max(sentences, 1),
                'words_per_minute': words_per_minute,
                'reading_time_minutes': word_count / 200  # Ortalama okuma hƒ±zƒ±
            },
            'audio_metadata': {
                'duration_seconds': duration_seconds,
                'duration_minutes': duration_minutes,
                'file_size_mb': audio_info.get('file_size_bytes', 0) / (1024 * 1024),
                'sample_rate': audio_info.get('sample_rate', 0),
                'channels': audio_info.get('channels', 1),
                'avg_db': audio_info.get('avg_db', -50)
            },
            'content_quality': {
                'speech_rate': 'Normal' if 120 <= words_per_minute <= 180 else 
                              'Hƒ±zlƒ±' if words_per_minute > 180 else 'Yava≈ü',
                'audio_quality': 'Y√ºksek' if audio_info.get('avg_db', -50) > -12 else
                                'Orta' if audio_info.get('avg_db', -50) > -20 else 'D√º≈ü√ºk'
            }
        })
        
        # Kelime frekansƒ± analizi
        if word_count > 10:
            # Temel T√ºrk√ße stopwords
            stopwords = {
                've', 'bir', 'bu', 'da', 'de', 'ile', 'i√ßin', 'olan', 'olarak', 
                'var', 'yok', 'gibi', 'kadar', 'daha', '√ßok', 'az', 'ya', 'ya da', 
                'ama', 'fakat', 'ancak', 'lakin', 'hem', 'ise', 'eƒüer', '≈üayet',
                'ki', 'mi', 'mƒ±', 'mu', 'm√º', 'ne', 'nasƒ±l', 'neden', 'ni√ßin',
                'ben', 'sen', 'o', 'biz', 'siz', 'onlar', 'bu', '≈üu', 'o'
            }
            
            # Kelimeleri temizle ve filtrele
            clean_words = []
            for word in words:
                clean_word = word.lower().strip('.,!?;:"()[]{}')
                if len(clean_word) > 2 and clean_word not in stopwords:
                    clean_words.append(clean_word)
            
            # En sƒ±k kullanƒ±lan kelimeleri bul
            word_freq = Counter(clean_words).most_common(10)
            
            ai_analysis['word_frequency'] = {
                'most_common_words': word_freq,
                'unique_word_count': len(set(clean_words)),
                'vocabulary_richness': len(set(clean_words)) / max(len(clean_words), 1)
            }
    
    except Exception as e:
        transcription_logger.warning(f"Enhancement error: {e}")
    
    return ai_analysis


def _display_results(uploaded_file, transcript_text: str, ai_analysis: Optional[Dict],
                    transcription_id: int, audio_info: Dict):
    """Sonu√ßlarƒ± g√∂ster"""
    
    st.markdown("---")
    
    # Transkripsiyon sonucu
    st.markdown(f"### üìù {uploaded_file.name} - {get_text('transcription_result_header')}")
    
    with st.container():
        st.markdown(f"""
        <div style="background: #1a1d23; padding: 1.5rem; border-radius: 10px; margin: 1rem 0; border-left: 4px solid #4a90e2;">
            <p style="line-height: 1.6; color: #fafafa; margin: 0;">{transcript_text}</p>
        </div>
        """, unsafe_allow_html=True)
    
    # AI Analiz sonu√ßlarƒ± (eƒüer varsa) - DETAYLI VERSIYON
    if ai_analysis:
        st.markdown("---")
        st.markdown(f"### ü§ñ {get_text('ai_analysis_results')}")
        
        # Detaylƒ± AI analiz g√∂sterimi
        _display_detailed_ai_analysis(ai_analysis, transcript_text)
    
    # Export se√ßenekleri
    st.markdown("---")
    _display_export_options(uploaded_file, transcript_text, ai_analysis, transcription_id, audio_info)


def _display_detailed_ai_analysis(ai_analysis: Dict[str, Any], transcript_text: str):
    """DETAYLI AI ANALƒ∞Z SONU√áLARINI G√ñSTERIR - T√úM √ñZELLƒ∞KLERƒ∞ KULLANIR"""
    
    if not ai_analysis:
        st.warning("‚ö†Ô∏è AI analiz verisi bulunamadƒ±")
        return
    
    # Hƒ∞Z METRIKLER OVERVIEW
    st.markdown("#### ‚ö° Hƒ±zlƒ± Bakƒ±≈ü")
    quick_col1, quick_col2, quick_col3, quick_col4, quick_col5 = st.columns(5)
    
    # Temel metrikleri al
    text_stats = ai_analysis.get('text_statistics', {})
    audio_meta = ai_analysis.get('audio_metadata', {})
    content_quality = ai_analysis.get('content_quality', {})
    
    with quick_col1:
        word_count = text_stats.get('word_count', len(transcript_text.split()))
        st.metric("üìù Kelime", f"{word_count:,}")
    
    with quick_col2:
        duration_min = audio_meta.get('duration_minutes', 0)
        st.metric("‚è±Ô∏è S√ºre", f"{duration_min:.1f}dk")
    
    with quick_col3:
        wpm = text_stats.get('words_per_minute', 0)
        if wpm > 0:
            st.metric("üó£Ô∏è Konu≈üma Hƒ±zƒ±", f"{wpm:.0f} kel/dk")
        else:
            st.metric("üó£Ô∏è Konu≈üma Hƒ±zƒ±", "N/A")
    
    with quick_col4:
        # Ana duyguyu √ßƒ±kar
        emotion = ai_analysis.get('emotion_analysis', 'Bilinmiyor')
        if isinstance(emotion, str) and emotion != "Duygusal analiz yapƒ±lamadƒ±":
            try:
                import json
                if emotion.strip().startswith('{'):
                    emotion_data = json.loads(emotion)
                    main_emotion = emotion_data.get('Ana Duygu', 'Bilinmiyor')
                else:
                    main_emotion = emotion.split()[0] if emotion else 'Bilinmiyor'
            except:
                main_emotion = 'Bilinmiyor'
        else:
            main_emotion = 'Bilinmiyor'
        
        st.metric("üí≠ Duygu", main_emotion)
    
    with quick_col5:
        # Ses kalitesi
        audio_quality = content_quality.get('audio_quality', 'Bilinmiyor')
        quality_color = "üü¢" if audio_quality == "Y√ºksek" else "üü°" if audio_quality == "Orta" else "üî¥"
        st.metric("üéµ Ses Kalitesi", f"{quality_color} {audio_quality}")
    
    # DETAYLI ANALIZ TABLARI
    st.markdown("---")
    
    # Tab'lƒ± detaylƒ± g√∂r√ºn√ºm
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üìä ƒ∞statistikler", 
        "üè∑Ô∏è Anahtar Kelimeler", 
        "üí≠ Duygu Analizi",
        "üìã √ñzet & Konular",
        "üéµ Ses Analizi",
        "üìà Kelime Analizi"
    ])
    
    with tab1:
        # DETAYLI ƒ∞STATƒ∞STƒ∞KLER
        st.markdown("#### üìä Detaylƒ± Metin ƒ∞statistikleri")
        
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        
        with stat_col1:
            st.markdown("**üìù Metin Yapƒ±sƒ±**")
            char_count = text_stats.get('character_count', len(transcript_text))
            sentence_count = text_stats.get('sentence_count', len([s for s in transcript_text.split('.') if s.strip()]))
            avg_words_per_sentence = text_stats.get('average_words_per_sentence', 0)
            
            st.write(f"‚Ä¢ **Karakter Sayƒ±sƒ±:** {char_count:,}")
            st.write(f"‚Ä¢ **C√ºmle Sayƒ±sƒ±:** {sentence_count:,}")
            st.write(f"‚Ä¢ **Ortalama Kelime/C√ºmle:** {avg_words_per_sentence:.1f}")
            
        with stat_col2:
            st.markdown("**‚è±Ô∏è Zaman Analizi**")
            reading_time = text_stats.get('reading_time_minutes', word_count / 200)
            
            st.write(f"‚Ä¢ **Okuma S√ºresi:** {reading_time:.1f} dakika")
            st.write(f"‚Ä¢ **Konu≈üma S√ºresi:** {duration_min:.1f} dakika")
            if duration_min > 0:
                efficiency = reading_time / duration_min
                st.write(f"‚Ä¢ **Verimlilik Oranƒ±:** {efficiency:.2f}x")
        
        with stat_col3:
            st.markdown("**üéØ Kalite Metrikleri**")
            speech_rate = content_quality.get('speech_rate', 'Bilinmiyor')
            
            # Kelime zenginliƒüi
            word_freq_data = ai_analysis.get('word_frequency', {})
            vocab_richness = word_freq_data.get('vocabulary_richness', 0)
            unique_words = word_freq_data.get('unique_word_count', 0)
            
            st.write(f"‚Ä¢ **Konu≈üma Hƒ±zƒ±:** {speech_rate}")
            st.write(f"‚Ä¢ **Benzersiz Kelime:** {unique_words:,}")
            st.write(f"‚Ä¢ **Kelime Zenginliƒüi:** {vocab_richness:.2%}")
    
    with tab2:
        # ANAHTAR KELƒ∞MELER VE KELƒ∞ME FREKANSI
        st.markdown("#### üè∑Ô∏è Anahtar Kelimeler ve Kelime Analizi")
        
        # AI'dan gelen anahtar kelimeler
        keywords = ai_analysis.get('keywords', [])
        if keywords:
            st.markdown("**üéØ AI Tespit Ettiƒüi Anahtar Kelimeler**")
            
            # Anahtar kelimeleri g√ºzel chip'ler halinde g√∂ster - tek seferde
            if keywords:
                st.markdown("**üéØ AI Tespit Ettiƒüi Anahtar Kelimeler**")
                
                # Kolayca okunabilir chip g√∂sterimi
                keywords_html = '<div style="background: #1a1d23; padding: 1rem; border-radius: 10px; border-left: 4px solid #4a90e2;">'
                
                for i, keyword in enumerate(keywords[:15]):  # ƒ∞lk 15 anahtar kelime
                    # Renkli gradient'ler
                    colors = [
                        'linear-gradient(135deg, #4a90e2, #667eea)',
                        'linear-gradient(135deg, #10b981, #34d399)', 
                        'linear-gradient(135deg, #f59e0b, #fbbf24)',
                        'linear-gradient(135deg, #ef4444, #f87171)',
                        'linear-gradient(135deg, #8b5cf6, #a78bfa)',
                        'linear-gradient(135deg, #f97316, #fb923c)',
                        'linear-gradient(135deg, #06b6d4, #22d3ee)',
                        'linear-gradient(135deg, #84cc16, #a3e635)'
                    ]
                    color = colors[i % len(colors)]
                    
                    keywords_html += f'''
                    <span style="display: inline-block; background: {color}; 
                                 color: white; padding: 6px 12px; margin: 3px; 
                                 border-radius: 15px; font-size: 0.85rem; font-weight: 500;
                                 box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
                        {keyword}
                    </span>'''
                
                keywords_html += '</div>'
                
                st.markdown(keywords_html, unsafe_allow_html=True)
                
                # Fazla kelime varsa bilgi g√∂ster
                if len(keywords) > 15:
                    st.info(f"üí° Toplam {len(keywords)} anahtar kelime bulundu. ƒ∞lk 15 tanesi g√∂steriliyor.")
            else:
                st.warning("‚ö†Ô∏è Anahtar kelime bulunamadƒ±")
        
        # Kelime frekansƒ± analizi
        st.markdown("---")
        word_freq_data = ai_analysis.get('word_frequency', {})
        most_common = word_freq_data.get('most_common_words', [])
        
        if most_common:
            st.markdown("**üìä En Sƒ±k Kullanƒ±lan Kelimeler**")
            
            freq_col1, freq_col2 = st.columns(2)
            
            with freq_col1:
                # ƒ∞lk 5 kelime
                for i, (word, count) in enumerate(most_common[:5]):
                    percentage = (count / word_count) * 100 if word_count > 0 else 0
                    st.metric(
                        label=f"üè∑Ô∏è {word}",
                        value=f"{count}x",
                        delta=f"{percentage:.1f}%"
                    )
            
            with freq_col2:
                # Kelime frekansƒ± grafiƒüi i√ßin basit g√∂sterim
                st.markdown("**üìà Kullanƒ±m Sƒ±klƒ±ƒüƒ±**")
                for word, count in most_common[:5]:
                    progress_value = count / most_common[0][1] if most_common else 0
                    st.progress(progress_value, text=f"{word}: {count}x")
    
    with tab3:
        # DUYGU ANALƒ∞Zƒ∞
        st.markdown("#### üí≠ Detaylƒ± Duygu Analizi")
        
        emotion_analysis = ai_analysis.get('emotion_analysis', '')
        if emotion_analysis and emotion_analysis != "Duygusal analiz yapƒ±lamadƒ±":
            try:
                # JSON formatƒ±nda geliyorsa parse et
                if emotion_analysis.strip().startswith('{'):
                    import json
                    emotion_data = json.loads(emotion_analysis)
                    
                    # Ana duygu ve detaylar
                    main_emotion = emotion_data.get('Ana Duygu', 'Bilinmiyor')
                    confidence = emotion_data.get('G√ºven', '0%')
                    tone = emotion_data.get('Ton', 'Bilinmiyor')
                    
                    # G√∂rsel duygu g√∂sterimi
                    emotion_color = _get_emotion_color(main_emotion)
                    confidence_num = int(confidence.replace('%', '')) if '%' in confidence else 0
                    
                    emo_col1, emo_col2 = st.columns([2, 1])
                    
                    with emo_col1:
                        st.markdown(f"""
                        <div style="background: {emotion_color}; padding: 2rem; border-radius: 15px; text-align: center; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <h2 style="color: white; margin-bottom: 1rem;">üòä {main_emotion}</h2>
                            <div style="background: rgba(255,255,255,0.2); padding: 1rem; border-radius: 10px;">
                                <p style="color: white; margin: 0; font-size: 1.1rem;"><strong>G√ºven Oranƒ±:</strong> {confidence}</p>
                                <p style="color: white; margin: 0.5rem 0 0 0; font-size: 1.1rem;"><strong>Genel Ton:</strong> {tone}</p>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                    
                    with emo_col2:
                        st.markdown("**üéØ Duygu Metrikleri**")
                        st.metric("üìä G√ºven Seviyesi", f"{confidence_num}%")
                        st.progress(confidence_num / 100, text=f"G√ºven: {confidence}")
                        
                        # Duygu kategorisi
                        if confidence_num >= 80:
                            certainty = "üü¢ √áok Y√ºksek"
                        elif confidence_num >= 60:
                            certainty = "üü° Y√ºksek"
                        elif confidence_num >= 40:
                            certainty = "üü† Orta"
                        else:
                            certainty = "üî¥ D√º≈ü√ºk"
                        
                        st.write(f"**Kesinlik:** {certainty}")
                        
                        # Sentiment skoru varsa g√∂ster
                        sentiment_score = ai_analysis.get('sentiment_score', None)
                        if sentiment_score is not None:
                            st.metric("üìà Sentiment Skoru", f"{sentiment_score:.2f}")
                
                else:
                    # D√ºz metin formatƒ±nda
                    st.markdown(f"""
                    <div style="background: #1a1d23; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #f59e0b;">
                        <h4 style="color: #f59e0b; margin-bottom: 1rem;">üí≠ Duygu Analizi</h4>
                        <p style="line-height: 1.6; color: #fafafa; margin: 0;">{emotion_analysis}</p>
                    </div>
                    """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Duygu analizi parse hatasƒ±: {e}")
                st.text(emotion_analysis)
        else:
            st.warning("‚ö†Ô∏è Duygu analizi bulunamadƒ± veya yapƒ±lamadƒ±")
    
    with tab4:
        # √ñZET VE KONULAR
        st.markdown("#### üìã ƒ∞√ßerik √ñzeti ve Konu Analizi")
        
        # √ñzet g√∂sterimi
        summary = ai_analysis.get('summary', '√ñzet bulunamadƒ±')
        st.markdown(f"""
        <div style="background: #1a1d23; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #10b981;">
            <h4 style="color: #10b981; margin-bottom: 1rem;">üìÑ ƒ∞√ßerik √ñzeti</h4>
            <p style="line-height: 1.8; color: #fafafa; margin: 0; font-size: 1.1rem;">{summary}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Ana konular
        topics = ai_analysis.get('topics', [])
        if topics:
            st.markdown("#### üéØ Ana Konular")
            
            # Konularƒ± grid halinde g√∂ster
            topic_cols = st.columns(min(len(topics), 3))
            for i, topic in enumerate(topics):
                with topic_cols[i % 3]:
                    st.markdown(f"""
                    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                               padding: 1.5rem; border-radius: 10px; text-align: center; 
                               margin: 0.5rem 0; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                        <h4 style="color: white; margin: 0;">{topic}</h4>
                    </div>
                    """, unsafe_allow_html=True)
        
        # ƒ∞√ßerik kategorisi
        content_category = ai_analysis.get('content_category', 'Bilinmiyor')
        language_detected = ai_analysis.get('language_detected', 'Bilinmiyor')
        
        cat_col1, cat_col2 = st.columns(2)
        with cat_col1:
            st.info(f"üè∑Ô∏è **ƒ∞√ßerik Kategorisi:** {content_category}")
        with cat_col2:
            st.info(f"üåç **Tespit Edilen Dil:** {language_detected}")
    
    with tab5:
        # SES ANALƒ∞Zƒ∞
        st.markdown("#### üéµ Ses Kalitesi ve Teknik Analiz")
        
        audio_col1, audio_col2, audio_col3 = st.columns(3)
        
        with audio_col1:
            st.markdown("**üîä Ses √ñzellikleri**")
            sample_rate = audio_meta.get('sample_rate', 0)
            channels = audio_meta.get('channels', 1)
            file_size_mb = audio_meta.get('file_size_mb', 0)
            
            st.write(f"‚Ä¢ **Sample Rate:** {sample_rate:,} Hz")
            st.write(f"‚Ä¢ **Kanallar:** {channels} ({'Stereo' if channels > 1 else 'Mono'})")
            st.write(f"‚Ä¢ **Dosya Boyutu:** {file_size_mb:.1f} MB")
        
        with audio_col2:
            st.markdown("**üìä Ses Kalitesi**")
            avg_db = audio_meta.get('avg_db', -50)
            audio_quality = content_quality.get('audio_quality', 'Bilinmiyor')
            
            # dB deƒüerine g√∂re kalite g√∂stergesi
            if avg_db > -12:
                db_color = "üü¢"
                db_desc = "M√ºkemmel"
            elif avg_db > -20:
                db_color = "üü°"
                db_desc = "ƒ∞yi"
            elif avg_db > -30:
                db_color = "üü†"
                db_desc = "Orta"
            else:
                db_color = "üî¥"
                db_desc = "D√º≈ü√ºk"
            
            st.write(f"‚Ä¢ **Ortalama dB:** {avg_db:.1f} dBFS")
            st.write(f"‚Ä¢ **Kalite:** {db_color} {db_desc}")
            st.write(f"‚Ä¢ **Genel Deƒüerlendirme:** {audio_quality}")
        
        with audio_col3:
            st.markdown("**‚ö° Performans Metrikleri**")
            speech_rate = content_quality.get('speech_rate', 'Bilinmiyor')
            
            # Konu≈üma hƒ±zƒ± deƒüerlendirmesi
            if wpm > 0:
                if wpm < 120:
                    rate_desc = "üêå Yava≈ü"
                elif wpm > 180:
                    rate_desc = "üèÉ Hƒ±zlƒ±"
                else:
                    rate_desc = "üëç Normal"
            else:
                rate_desc = "‚ùì Bilinmiyor"
            
            st.write(f"‚Ä¢ **Konu≈üma Hƒ±zƒ±:** {rate_desc}")
            st.write(f"‚Ä¢ **Kelime/Dakika:** {wpm:.0f}" if wpm > 0 else "‚Ä¢ **Kelime/Dakika:** N/A")
            st.write(f"‚Ä¢ **Deƒüerlendirme:** {speech_rate}")
    
    with tab6:
        # KELƒ∞ME ANALƒ∞Zƒ∞ VE ƒ∞STATƒ∞STƒ∞KLER
        st.markdown("#### üìà Geli≈ümi≈ü Kelime Analizi")
        
        word_freq_data = ai_analysis.get('word_frequency', {})
        
        if word_freq_data:
            vocab_richness = word_freq_data.get('vocabulary_richness', 0)
            unique_words = word_freq_data.get('unique_word_count', 0)
            most_common = word_freq_data.get('most_common_words', [])
            
            # Kelime zenginliƒüi g√∂sterimi
            vocab_col1, vocab_col2 = st.columns(2)
            
            with vocab_col1:
                st.markdown("**üìä Kelime Zenginliƒüi**")
                st.metric("üè∑Ô∏è Benzersiz Kelime", f"{unique_words:,}")
                st.metric("üìà Zenginlik Oranƒ±", f"{vocab_richness:.2%}")
                
                # Zenginlik deƒüerlendirmesi
                if vocab_richness > 0.5:
                    richness_desc = "üåü √áok Zengin"
                elif vocab_richness > 0.3:
                    richness_desc = "üëç Zengin"
                elif vocab_richness > 0.2:
                    richness_desc = "üìù Orta"
                else:
                    richness_desc = "üìâ Sƒ±nƒ±rlƒ±"
                
                st.info(f"**Deƒüerlendirme:** {richness_desc}")
            
            with vocab_col2:
                st.markdown("**üî• En Pop√ºler Kelimeler**")
                
                # Top 10 kelimeleri tablo halinde
                if most_common:
                    for i, (word, count) in enumerate(most_common[:8], 1):
                        percentage = (count / word_count) * 100 if word_count > 0 else 0
                        st.write(f"{i}. **{word}** - {count}x ({percentage:.1f}%)")
        
        # Kelime uzunluƒüu analizi
        words = transcript_text.split()
        if words:
            avg_word_length = sum(len(word.strip('.,!?;:"()[]{}')) for word in words) / len(words)
            long_words = len([word for word in words if len(word.strip('.,!?;:"()[]{}')) > 6])
            
            length_col1, length_col2 = st.columns(2)
            
            with length_col1:
                st.markdown("**üìè Kelime Uzunluƒüu Analizi**")
                st.metric("üìê Ortalama Uzunluk", f"{avg_word_length:.1f} harf")
                st.metric("üìö Uzun Kelime Sayƒ±sƒ±", f"{long_words:,}")
            
            with length_col2:
                # Basit kelime uzunluƒüu daƒüƒ±lƒ±mƒ±
                st.markdown("**üìä Uzunluk Daƒüƒ±lƒ±mƒ±**")
                short_words = len([w for w in words if len(w.strip('.,!?;:"()[]{}')) <= 4])
                medium_words = len([w for w in words if 5 <= len(w.strip('.,!?;:"()[]{}')) <= 6])
                
                st.write(f"‚Ä¢ **Kƒ±sa (‚â§4 harf):** {short_words:,}")
                st.write(f"‚Ä¢ **Orta (5-6 harf):** {medium_words:,}")
                st.write(f"‚Ä¢ **Uzun (>6 harf):** {long_words:,}")
    
    # Ek bilgi notu (expander kaldƒ±rƒ±ldƒ± - nested expander hatasƒ± nedeniyle)
    st.markdown("---")
    st.info("ÔøΩ **Detaylƒ± AI Analiz:** Yukarƒ±daki sekmeli g√∂r√ºn√ºmde t√ºm analiz sonu√ßlarƒ± bulunmaktadƒ±r.")


def _clean_for_json(obj):
    """Nesneyi JSON serializable hale getirir (numpy array'leri vs. temizler)"""
    import numpy as np
    
    if isinstance(obj, dict):
        return {key: _clean_for_json(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [_clean_for_json(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()  # numpy array'i listeye √ßevir
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()  # numpy scalar'ƒ± Python scalar'a √ßevir
    elif hasattr(obj, 'dtype'):  # Diƒüer numpy t√ºrleri
        return str(obj)
    else:
        return obj


def _auto_save_pdf_report(uploaded_file, transcript_text: str, ai_analysis: Optional[Dict],
                         transcription_id: int, audio_info: Dict):
    """Otomatik PDF raporu olu≈üturur ve 'data' klas√∂r√ºne kaydeder - export butonu olmadan"""
    
    st.markdown("---")
    st.markdown("### üìÑ Otomatik PDF Raporu")
    
    # ReportLab kontrol√º
    if not _install_reportlab_if_needed():
        st.error("‚ùå PDF raporu olu≈üturulamadƒ±: ReportLab k√ºt√ºphanesi bulunamadƒ±")
        return
    
    with st.spinner("üîÑ Detaylƒ± AI analiz raporu PDF olarak hazƒ±rlanƒ±yor..."):
        try:
            pdf_path = _create_pdf_report(
                uploaded_file, 
                transcript_text, 
                ai_analysis, 
                transcription_id, 
                audio_info
            )
            
            if pdf_path:
                st.success(f"‚úÖ PDF raporu otomatik olarak kaydedildi!")
                
                # PDF bilgileri g√∂ster
                pdf_file = Path(pdf_path)
                file_size = pdf_file.stat().st_size / 1024  # KB
                
                # Bilgi kartƒ± - export butonu olmadan
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #4a90e2, #667eea); 
                           padding: 2rem; border-radius: 15px; margin: 1rem 0; 
                           box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <h3 style="color: white; margin: 0 0 1rem 0; text-align: center;">
                        ÔøΩ PDF Raporu Hazƒ±r!
                    </h3>
                    <div style="background: rgba(255,255,255,0.2); padding: 1.5rem; border-radius: 10px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1rem; color: white;">
                            <div style="text-align: center;">
                                <h4 style="margin: 0; font-size: 2rem;">üìÑ</h4>
                                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">PDF Boyutu</p>
                                <p style="margin: 0; font-weight: bold;">{file_size:.1f} KB</p>
                            </div>
                            <div style="text-align: center;">
                                <h4 style="margin: 0; font-size: 2rem;">üìÇ</h4>
                                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Kayƒ±t Konumu</p>
                                <p style="margin: 0; font-weight: bold;">data/</p>
                            </div>
                            <div style="text-align: center;">
                                <h4 style="margin: 0; font-size: 2rem;">‚è∞</h4>
                                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Olu≈üturma Zamanƒ±</p>
                                <p style="margin: 0; font-weight: bold;">{datetime.now().strftime("%H:%M:%S")}</p>
                            </div>
                        </div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # Dosya yolu bilgisi
                st.info(f"""
                **üìÅ Dosya Yolu:** `{pdf_path}`
                
                **üìã ƒ∞√ßerik:**
                ‚Ä¢ Dosya bilgileri ve teknik detaylar  
                ‚Ä¢ Tam transkripsiyon metni
                ‚Ä¢ Detaylƒ± AI analiz sonu√ßlarƒ±
                ‚Ä¢ ƒ∞statistiksel veriler ve tablolar
                ‚Ä¢ Anahtar kelimeler ve duygu analizi
                ‚Ä¢ T√ºrk√ße karakter uyumlu format
                """)
                
                # Ba≈üarƒ± mesajƒ±
                st.balloons()
                
            else:
                st.error("‚ùå PDF raporu olu≈üturulamadƒ±")
                
        except Exception as e:
            st.error(f"‚ùå PDF olu≈üturma hatasƒ±: {str(e)}")
            transcription_logger.error(f"Auto PDF creation error: {e}")
    
    # Ek bilgi - export butonu yok
    st.markdown("---")
    st.info("""
    ü§ñ **Otomatik PDF Kaydetme Sistemi:**
    
    ‚Ä¢ **Tam Otomatik:** Her transkripsiyon i≈üleminden sonra otomatik olu≈üturulur
    ‚Ä¢ **Kayƒ±t Konumu:** Proje klas√∂r√ºnde `data/` dizini
    ‚Ä¢ **T√ºrk√ße Destek:** T√ºrk√ße karakterler uyumlu format
    ‚Ä¢ **Detaylƒ± ƒ∞√ßerik:** AI analizi, istatistikler, tablolar
    ‚Ä¢ **Benzersiz ƒ∞sim:** Tarih-saat damgasƒ± ile √ßakƒ±≈üma √∂nlenir
    ‚Ä¢ **Export Butonu Yok:** Otomatik kaydetme, manual export gerekmez
    """)


def _display_export_options(uploaded_file, transcript_text: str, ai_analysis: Optional[Dict],
                           transcription_id: int, audio_info: Dict):
    """Otomatik PDF kaydetme - export se√ßenekleri tamamen kaldƒ±rƒ±ldƒ±"""
    
    # Otomatik PDF raporu olu≈ütur - buton olmadan
    _auto_save_pdf_report(uploaded_file, transcript_text, ai_analysis, transcription_id, audio_info)


def _get_emotion_color(emotion: str) -> str:
    """Duyguya g√∂re renk d√∂nd√ºr√ºr"""
    emotion_colors = {
        'Pozitif': 'linear-gradient(135deg, #10b981, #047857)',
        'Negatif': 'linear-gradient(135deg, #ef4444, #dc2626)',
        'N√∂tr': 'linear-gradient(135deg, #6b7280, #4b5563)',
        'Mutlu': 'linear-gradient(135deg, #f59e0b, #d97706)',
        '√úzg√ºn': 'linear-gradient(135deg, #3b82f6, #1d4ed8)',
        '√ñfkeli': 'linear-gradient(135deg, #ef4444, #991b1b)',
        'Heyecanlƒ±': 'linear-gradient(135deg, #8b5cf6, #7c3aed)',
        'Sakin': 'linear-gradient(135deg, #06b6d4, #0891b2)',
        'Gergin': 'linear-gradient(135deg, #f97316, #ea580c)',
        'Rahat': 'linear-gradient(135deg, #22c55e, #16a34a)',
    }
    return emotion_colors.get(emotion, 'linear-gradient(135deg, #6b7280, #4b5563)')


def _get_sentiment_color(score: float) -> str:
    """Sentiment skoruna g√∂re renk d√∂nd√ºr√ºr"""
    if score >= 0.5:
        return 'linear-gradient(135deg, #10b981, #047857)'  # Pozitif - ye≈üil
    elif score <= -0.5:
        return 'linear-gradient(135deg, #ef4444, #dc2626)'  # Negatif - kƒ±rmƒ±zƒ±
    else:
        return 'linear-gradient(135deg, #f59e0b, #d97706)'  # N√∂tr - turuncu


# CSS stilleri - sadece bu mod√ºl i√ßin
def apply_upload_tab_styles():
    """Upload tab i√ßin √∂zel CSS stilleri"""
    st.markdown("""
    <style>
        /* Upload specific styles */
        .upload-file-container {
            background: #1a1d23;
            border: 2px dashed #4a90e2;
            border-radius: 12px;
            padding: 2rem;
            text-align: center;
            margin: 1rem 0;
            transition: all 0.3s ease;
        }
        
        .upload-file-container:hover {
            border-color: #f59e0b;
            background: #2d3748;
        }
        
        .file-info-card {
            background: #1a1d23;
            border-radius: 8px;
            padding: 1rem;
            margin: 0.5rem 0;
            border-left: 3px solid #4a90e2;
        }
        
        .result-card {
            background: #1a1d23;
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1rem 0;
            border-left: 4px solid #4a90e2;
        }
        
        /* Enhanced analysis cards */
        .analysis-card {
            background: linear-gradient(135deg, #1a1d23 0%, #2d3748 100%);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1rem 0;
            border: 1px solid rgba(74, 144, 226, 0.3);
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        /* Tab styling */
        .stTabs [data-baseweb="tab-list"] {
            gap: 8px;
        }
        
        .stTabs [data-baseweb="tab"] {
            background-color: #2d3748;
            border-radius: 8px;
            color: #cbd5e0;
            padding: 0.5rem 1rem;
        }
        
        .stTabs [aria-selected="true"] {
            background-color: #4a90e2;
            color: white;
        }
    </style>
    """, unsafe_allow_html=True)
